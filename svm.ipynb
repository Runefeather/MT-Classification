{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('mt_classif': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6aecabaf619af1d96420ea5a51883a7b88da7023682e970e87bafdc65f71c809"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "import re\n",
    "\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(x):\n",
    "    if(x == 'H'):\n",
    "        return 1\n",
    "    return -1 \n",
    "\n",
    "# Determinants\n",
    "def count_DT(x):\n",
    "    doc = nlp(x)\n",
    "    return round(len([w for w in doc if w.pos_ == \"DET\"])/len_sentence(x), 3)\n",
    "\n",
    "# Pronouns\n",
    "def count_PRON(x):\n",
    "    doc = nlp(x)\n",
    "    return round(len([w for w in doc if w.pos_ == \"PRON\"])/len_sentence(x), 3)\n",
    "\n",
    "# Punctuation marks\n",
    "def count_PUNC(x):\n",
    "    doc = nlp(x)\n",
    "    return round(len([w for w in doc if w.pos_ == \"PUNCT\"])/len_sentence(x), 3)\n",
    "\n",
    "# Auxilliary verbs\n",
    "def count_AUX(x):\n",
    "    doc = nlp(x)\n",
    "    return round(len([w for w in doc if w.pos_ == \"AUX\"])/len_sentence(x), 3)\n",
    "\n",
    "# Prepositions\n",
    "def count_PREP(x):\n",
    "    doc = nlp(x)\n",
    "    return round(len([w for w in doc if w.pos_ == \"ADP\"])/len_sentence(x), 3)\n",
    "\n",
    "# Predeterminers\n",
    "def count_PDT(x):\n",
    "    doc = nlp(x)\n",
    "    return round(len([w for w in doc if w.tag_ == \"PDT\"])/len_sentence(x), 3)\n",
    "\n",
    "# Noun phrases\n",
    "def count_NP(x):\n",
    "    doc = nlp(x)\n",
    "    return round(len([n for n in doc.noun_chunks])/len_sentence(x), 3)\n",
    "\n",
    "# Indirect objects\n",
    "def count_IOBJ(x):\n",
    "    doc = nlp(x)\n",
    "    return round(len([w for w in doc if w.dep_ == \"iobj\"])/len_sentence(x), 3)\n",
    "\n",
    "# Nominal subjects\n",
    "def count_NSUB(x):\n",
    "    doc = nlp(x)\n",
    "    return round(len([w for w in doc if w.dep_ == \"nsubj\"])/len_sentence(x), 3)\n",
    "\n",
    "# Coordinating Conjunctions\n",
    "def count_CONJ(x):\n",
    "    doc = nlp(x)\n",
    "    return round(len([w for w in doc if w.pos_ == \"CCONJ\"])/len_sentence(x), 3)\n",
    "\n",
    "# Length of sentence\n",
    "def len_sentence(x):\n",
    "    doc = nlp(x)\n",
    "    return len([token for token in doc if not token.is_punct | token.is_space])\n",
    "\n",
    "def create_new_labels(df):\n",
    "    # Reference\n",
    "    df['Ref_DT'] = df['Reference'].apply(count_DT)\n",
    "    df['Ref_NP'] = df['Reference'].apply(count_NP)\n",
    "    df['Ref_pron'] = df['Reference'].apply(count_PRON)\n",
    "    df['Ref_punc'] = df['Reference'].apply(count_PUNC)\n",
    "    df['Ref_aux'] = df['Reference'].apply(count_AUX)\n",
    "    df['Ref_prep'] = df['Reference'].apply(count_PREP)\n",
    "    df['Ref_predet'] = df['Reference'].apply(count_PDT)\n",
    "    df['Ref_iobj'] = df['Reference'].apply(count_IOBJ)\n",
    "    df['Ref_nsub'] = df['Reference'].apply(count_NSUB)\n",
    "    df['Ref_cconj'] = df['Reference'].apply(count_CONJ)\n",
    "    df['Ref_len'] = df['Reference'].apply(len_sentence)\n",
    "\n",
    "    # # Candidate\n",
    "    df['Cand_DT'] = df['Candidate'].apply(count_DT)\n",
    "    df['Cand_NP'] = df['Candidate'].apply(count_NP)\n",
    "    df['Cand_pron'] = df['Candidate'].apply(count_PRON)\n",
    "    df['Cand_punc'] = df['Candidate'].apply(count_PUNC)\n",
    "    df['Cand_aux'] = df['Candidate'].apply(count_AUX)\n",
    "    df['Cand_prep'] = df['Candidate'].apply(count_PREP)\n",
    "    df['Cand_predet'] = df['Candidate'].apply(count_PDT)\n",
    "    df['Cand_iobj'] = df['Candidate'].apply(count_IOBJ)\n",
    "    df['Cand_nsub'] = df['Candidate'].apply(count_NSUB)\n",
    "    df['Cand_cconj'] = df['Candidate'].apply(count_CONJ)\n",
    "    df['Cand_len'] = df['Candidate'].apply(len_sentence)\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_data(f):\n",
    "    out_dict = {}\n",
    "    i = 0\n",
    "    counter = 0\n",
    "    headers = ['Original', 'Reference', 'Candidate', 'BLEU', 'Label']\n",
    "    # Read file into a nested dictionary\n",
    "    with io.open(f, 'r', encoding='utf-8') as file:\n",
    "        temp_dict = {}\n",
    "        for line in file:\n",
    "            if line.strip():\n",
    "                temp_dict[headers[counter]] = line.strip()\n",
    "                counter += 1\n",
    "            else:\n",
    "                out_dict[i] = temp_dict\n",
    "                i += 1\n",
    "                temp_dict = {}\n",
    "                counter = 0\n",
    "    df = pd.DataFrame(data=out_dict).T\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "\n",
    "def convert_chinese(x, vectorizer):\n",
    "    matrix = vectorizer.fit_transform(x)\n",
    "    converted = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "    return converted, vectorizer.get_feature_names()\n",
    "\n",
    "def convert_english(x, count_vect):\n",
    "    x = [a.lower() for a in x] \n",
    "    x = [re.sub(r'\\d+', '', a) for a in x]\n",
    "    eng_matrix = count_vect.fit_transform(x)\n",
    "    eng_converted = pd.DataFrame(eng_matrix.toarray(), columns=count_vect.get_feature_names())\n",
    "    return eng_converted, count_vect.get_feature_names()\n",
    "\n",
    "def test_convert_chinese(x, vectorizer):\n",
    "    matrix = vectorizer.transform(x)\n",
    "    converted = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "    return converted, vectorizer.get_feature_names()\n",
    "\n",
    "def test_convert_english(x, count_vect):\n",
    "    x = [a.lower() for a in x] \n",
    "    x = [re.sub(r'\\d+', '', a) for a in x]\n",
    "    eng_matrix = count_vect.transform(x)\n",
    "    eng_converted = pd.DataFrame(eng_matrix.toarray(), columns=count_vect.get_feature_names())\n",
    "    return eng_converted, count_vect.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from train and text files\n",
    "train = extract_data('train.txt')\n",
    "train_labels = train.pop('Label').apply(convert_label)\n",
    "\n",
    "# Performs the creation of labels based on POS\n",
    "train = create_new_labels(train)\n",
    "\n",
    "stop_words = ['。', '，']\n",
    "chi_vect = CountVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)\n",
    "eng_vect = CountVectorizer()\n",
    "\n",
    "# Converts words in Chinese and English to one hot vectors in TRAIN\n",
    "orig_df, orig_cols = convert_chinese(list(train.pop('Original')), chi_vect)\n",
    "ref_df, ref_cols = convert_english(list(train.pop('Reference')), eng_vect)\n",
    "cand_df, cand_cols = test_convert_english(list(train.pop('Candidate')), eng_vect)\n",
    "\n",
    "train[orig_cols] = orig_df\n",
    "train[ref_cols] = ref_df\n",
    "train[cand_cols] = cand_df\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "583 6537\n(583,)\n(173, 6526)\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the training data.\n",
    "train_size,num_features = train_tfidf.shape\n",
    "print(train_size, num_features)\n",
    "print(train_labels.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(173, 6537)\n"
     ]
    }
   ],
   "source": [
    "# Converts words in Chinese and English to one hot vectors in TEST\n",
    "test = extract_data('test.txt')\n",
    "test_labels = test.pop('Label').apply(convert_label)\n",
    "test = create_new_labels(test)\n",
    "\n",
    "test_orig_df, test_orig_cols = test_convert_chinese(list(test.pop('Original')), chi_vect)\n",
    "test_ref_df, test_ref_cols = test_convert_english(list(test.pop('Reference')), eng_vect)\n",
    "test_cand_df, test_cand_cols = test_convert_english(list(test.pop('Candidate')), eng_vect)\n",
    "test[test_orig_cols] = test_orig_df\n",
    "test[test_ref_cols] = test_ref_df\n",
    "test[test_cand_cols] = test_cand_df\n",
    "\n",
    "test_tfidf = tfidf_transformer.transform(test)\n",
    "print(test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average F1-score of the two classes: 0.8192633033397364\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='poly', gamma=2)\n",
    "SVM.fit(train_tfidf,train_labels)\n",
    "\n",
    "pred_labels = SVM.predict(test_tfidf)\n",
    "print(\"Average F1-score of the two classes:\", sum(sklearn.metrics.f1_score(test_labels, pred_labels, average=None)/2))\n",
    "\n"
   ]
  }
 ]
}